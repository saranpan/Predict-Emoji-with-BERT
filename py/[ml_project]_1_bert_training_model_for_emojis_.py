# -*- coding: utf-8 -*-
"""[ML project] #1 BERT Training model for Emojis .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ffq_bscV74tVQzxIuU_soh076sGufUJk

SETUP
"""

!pip install kora kaggle --quiet

from kora import kaggle

print("If failed, download Kaggle API to your drive first")

import pandas as pd, numpy as np,seaborn as sns
import tensorflow as tf, torch
import sys

# DON"T RUN THE CODE IF YOU DIDN"T ENABLE GPU YET
if tf.test.gpu_device_name() == '':
  print("make sure to turn on GPU")
  sys.exit()

else:
  print('GPU enabled')

"""---

import data
"""

kaggle.search('emotions-dataset-for-nlp')

kaggle.download('praveengovi/emotions-dataset-for-nlp')

"""

---

"""

col_name = ['text','emoji']

train  = pd.read_csv('/content/train.txt',delimiter=';',header=None,names=col_name)
val =  pd.read_csv('/content/val.txt',delimiter=';',header=None,names=col_name)
test =  pd.read_csv('/content/test.txt',delimiter=';',header=None,names=col_name)

"""

---

"""

train

def miss_val(df):
  if df.isna().sum().all() == 0 :
    return 'no missing value'
  else:
    return 'have missing value'
    
print(f'train : {miss_val(train)}')
print(f'val : {miss_val(val)}')
print(f'test : {miss_val(test)}')

print(f'training set size : {len(train)}')

pd.value_counts(train['emoji']).plot.bar()

print(f'training set size : {len(val)}')

pd.value_counts(val['emoji']).plot.bar()

print(f'training set size : {len(test)}')

pd.value_counts(test['emoji']).plot.bar()

train.isna().sum().all() == 1

miss_val(train)

"""ALL CHECKED!

---

TRAINING DATA
"""

df_train = train.copy()

df_val = val.copy()



# Make label numerical

possible_labels = df_train.emoji.unique()

label_dict = {}
for index, possible_label in enumerate(possible_labels):
    label_dict[possible_label] = index

label_dict

# replace emoji ny the label number
df_train['label'] = df_train.emoji.replace(label_dict)
df_val['label'] = df_val.emoji.replace(label_dict)

df_train

df_train.head()

df_val.head()

"""

---

"""

# Split the data (both train and val)

(X_train , y_train) = (df_train.text , df_train.emoji)
(X_val , y_val) = (df_val.text , df_val.emoji)

#Check the count of both train and val

label_count = pd.concat([train,val],axis=0)
label_count.reset_index(drop=True,inplace=True)
label_count['label'] = label_count.emoji.replace(label_dict)
label_count['data_type'] = ['not_set']*label_count.shape[0]

label_count.loc[:16000, 'data_type'] = 'train'
label_count.loc[16000:, 'data_type'] = 'val'

label_count.groupby(['emoji', 'label', 'data_type']).count()

#We don't use anymore 
del label_count

"""CHECKED"""



"""---

BERT
"""

!pip install transformers --quiet

import torch
from tqdm.notebook import tqdm

#TOKENIZER
from transformers import BertTokenizer
from torch.utils.data import TensorDataset


from transformers import BertForSequenceClassification

# construct BERT tokenizer ( known as WordPiece ) 

# tokenizer: split raw text into tokens (represent in numeric data)

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', 
                                          do_lower_case=True)

"""NEXT STATION: https://github.com/susanli2016/NLP-with-Python/blob/master/Text_Classification_With_BERT.ipynb

Batch encode plus VS Batch encode : https://stackoverflow.com/questions/61708486/whats-difference-between-tokenizer-encode-and-tokenizer-encode-plus-in-hugging
"""

""" encode_plus support encoding more than 1 sentence """

"""we decide to return 3 things from the tokenizer : token vector, attention mask, label in PyTorch form """
# tokenize the training data
encoded_data_train = tokenizer.batch_encode_plus(
    batch_text_or_text_pairs = df_train['text'].values, 
    
    #special token used to make arrays of tokens the same size for batching purpose (which fulfill by zero element when the size is small)
    add_special_tokens=True, 

    # we want attention mask too, to don't fully show text all text, but we hide some
    return_attention_mask=True, 

    #Padding the returned sequence to fit the max length (maybe if the returned sequence is larger than max length)
    pad_to_max_length=True, 

    #limit the size of numerical token
    max_length=256, 

    #return the numerical tokens in form of PyTorch
    return_tensors='pt'
)

# tokenize the validation data
encoded_data_val = tokenizer.batch_encode_plus(
    batch_text_or_text_pairs = df_val['text'].values, 
    add_special_tokens=True, 
    return_attention_mask=True, 
    pad_to_max_length=True, 
    max_length=256, 
    return_tensors='pt'
)




#our token vector
input_ids_train = encoded_data_train['input_ids']

#mask for each instance
attention_masks_train = encoded_data_train['attention_mask']

#transform array to Pytorch object
labels_train = torch.tensor(df_train['label'].values)

input_ids_val = encoded_data_val['input_ids']
attention_masks_val = encoded_data_val['attention_mask']
labels_val = torch.tensor(df_val['label'].values)


#We want tensor dataset, cuz one feature can store matrix ( 1d+ tensor) instead of just scalar ( 1d tensor) )
dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)
dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)

"""

---

"""

#token vector of each instance w/ size 16k (training size)

encoded_data_train['input_ids']

#show how it masks for each sentence

encoded_data_train['attention_mask']

#transform array to Pytorch object

torch.tensor(df_train['label'].values)

"""---

OUR BERT MODEL
"""

model = BertForSequenceClassification.from_pretrained("bert-base-uncased",
                                                      #We want them to return the label of each emojis (6) 
                                                      num_labels=len(label_dict),

                                                      # We don't want both attention and hidden state as output
                                                      output_attentions=False,
                                                      output_hidden_states=False)

"""---

DATALOADER (Random sampling, seq samp) : https://www.scottcondron.com/jupyter/visualisation/audio/2020/12/02/dataloaders-samplers-collate.html#DataLoaders-are-magic.
"""

from torch.utils.data import DataLoader, RandomSampler, SequentialSampler

#for limited memory #next:32
batch_size = 3

# randomly shuffle the data; 
dataloader_train = DataLoader(dataset_train, 
                              sampler=RandomSampler(dataset_train), 
                              batch_size=batch_size)

# order the data
dataloader_validation = DataLoader(dataset_val, 
                                   sampler=SequentialSampler(dataset_val), 
                                   batch_size=batch_size)

# check out the sequence of indices/keys used in data loading (training)

"""
for i in dataloader_train.sampler:
  print(i)
"""

# (validation)

"""
for i in dataloader_validation.sampler:
  print(i)
"""

"""---

Numerical stable (for eps) : https://nhigham.com/2020/08/04/what-is-numerical-stability/
"""

from transformers import AdamW, get_linear_schedule_with_warmup

#optimizer can tell the network how to change its weights.

#Adam adapt the learning rate, preventing the loss function from diverge
optimizer = AdamW(model.parameters(),
                  
                  #learning rate
                  lr=1e-5, 

                  #acceptable error
                  eps=1e-8)

"""given lr as learning rate, N as number of Epochs

<fieldset>

- The first epoch use 1*lr/N as learning rate  (This epoch is the most large update weight, since the slope is extremely high.. so it updates a big parameter)

- The second epoch use 2*lr/N as learning rate

- The ith epoch use i*lr/N as learning rate
</fieldset>

So, the learning rate will start from nearly zero to lr that you set

warmup to prevent early overfitting (supernatural belief, and exponentially update the weight)

This help limit the learning rate of the early epoch

The first epoch use 1*lr/N as learning rate

Prevevnt this worst case : If your shuffled data happens to <u>include a cluster of related, strongly-featured observations</u>, your model's initial training can skew badly toward those features -- or worse, toward incidental features that aren't truly related to the topic at all.

ref: [Warmup learning rate](https://stackoverflow.com/questions/55933867/what-does-learning-rate-warm-up-mean/55942518)
"""

# (linearly) learning rate warm up

epochs = 5 #@param {type:"slider", min:1, max:100, step:1}

scheduler = get_linear_schedule_with_warmup(optimizer, 
                                            num_warmup_steps=0,
                                            num_training_steps=len(dataloader_train)*epochs)

#our evaluation


from sklearn.metrics import f1_score

def f1_score_func(preds, labels):
    preds_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return f1_score(labels_flat, preds_flat, average='weighted')

# easier to see what's wrong with the wrong prediction of each class
def accuracy_per_class(preds, labels):
    label_dict_inverse = {v: k for k, v in label_dict.items()}
    
    preds_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()

    for label in np.unique(labels_flat):
        y_preds = preds_flat[labels_flat==label]
        y_true = labels_flat[labels_flat==label]
        print(f'Class: {label_dict_inverse[label]}')
        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\n')

"""---

#Training Loops
"""

#Set Random seed (for predictable random result)

import random

seed_val = 17
random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

# Prepare device (expect GPU called CUDA)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

print(f'your current device is {device} \n\n')

# Using model to evaluate and predict the model

def evaluate(dataloader_val):

    model.eval()
    
    loss_val_total = 0
    predictions, true_vals = [], []
    
    for batch in dataloader_val:
        
        batch = tuple(b.to(device) for b in batch)
        
        inputs = {'input_ids':      batch[0],
                  'attention_mask': batch[1],
                  'labels':         batch[2],
                 }

        with torch.no_grad():        
            outputs = model(**inputs)
            
        loss = outputs[0]
        logits = outputs[1]
        loss_val_total += loss.item()

        logits = logits.detach().cpu().numpy()
        label_ids = inputs['labels'].cpu().numpy()
        predictions.append(logits)
        true_vals.append(label_ids)
    
    loss_val_avg = loss_val_total/len(dataloader_val) 
    
    predictions = np.concatenate(predictions, axis=0)
    true_vals = np.concatenate(true_vals, axis=0)
            
    return loss_val_avg, predictions, true_vals

for i in data_te:
  print(i)

for epoch in tqdm(range(1, epochs+1)):
    
    model.train()
    
    loss_train_total = 0

    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)
    for batch in progress_bar:

        model.zero_grad()
        
        batch = tuple(b.to(device) for b in batch)
        
        inputs = {'input_ids':      batch[0],
                  'attention_mask': batch[1],
                  'labels':         batch[2],
                 }       

        outputs = model(**inputs)
        
        loss = outputs[0]
        loss_train_total += loss.item()
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        optimizer.step()
        scheduler.step()
        
        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})
         

    #Hop the model    
    torch.save(model.state_dict(), f'/content/finetuned_BERT_epoch_{epoch}.model')
        
    tqdm.write(f'\nEpoch {epoch}')
    
    loss_train_avg = loss_train_total/len(dataloader_train)            
    tqdm.write(f'Training loss: {loss_train_avg}')
    
    val_loss, predictions, true_vals = evaluate(dataloader_validation)
    val_f1 = f1_score_func(predictions, true_vals)
    tqdm.write(f'Validation loss: {val_loss}')
    tqdm.write(f'F1 Score (Weighted): {val_f1}')

pwd

"""#torch.save(model.state_dict(), f'content/finetuned_BERT_epoch_{1}.model')
torch.save(model.state_dict(), f'/content/finetuned_BERT_epoch_{1}.model')


tqdm.write(f'\nEpoch {epoch}')

loss_train_avg = loss_train_total/len(dataloader_train)            
tqdm.write(f'Training loss: {loss_train_avg}')
    
val_loss, predictions, true_vals = evaluate(dataloader_validation)
val_f1 = f1_score_func(predictions, true_vals)
tqdm.write(f'Validation loss: {val_loss}')
tqdm.write(f'F1 Score (Weighted): {val_f1}')

---

''

# Load and Evaluate the model
"""

!pip install transformers --quiet

import torch
from tqdm.notebook import tqdm

#TOKENIZER
from transformers import BertTokenizer
from torch.utils.data import TensorDataset


from transformers import BertForSequenceClassification

"""
import warnings
warnings.filterwarnings(action='once')
"""

model = BertForSequenceClassification.from_pretrained("bert-base-uncased",
                                                      num_labels=len(label_dict),
                                                      output_attentions=False,
                                                      output_hidden_states=False)

model.to(device)

#Download BERT model at epoch 1
model.load_state_dict(torch.load(f'/content/finetuned_BERT_epoch_{1}.model', map_location=torch.device('cpu')))

_, predictions, true_vals = evaluate(dataloader_validation)

accuracy_per_class(predictions, true_vals)

"""

---

"""



"""---

Test set

PREPROCESSING TEST SET
"""

df_test = test.copy()

df_test['label']



df_test['label'] = df_test.emoji.replace(label_dict)

encoded_data_test = tokenizer.batch_encode_plus(
    batch_text_or_text_pairs = df_test['text'].values, 
    add_special_tokens=True, 
    return_attention_mask=True, 
    pad_to_max_length=True, 
    max_length=256, 
    return_tensors='pt'
)


input_ids_test = encoded_data_test['input_ids']
attention_masks_test = encoded_data_test['attention_mask']
labels_test = torch.tensor(df_test['label'].values)

dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)

dataloader_test = DataLoader(dataset_test, 
                                   sampler=SequentialSampler(dataset_test), 
                                   batch_size=batch_size)

df_test['text']

string = np.array(['I love you '])


encoded_string = tokenizer.batch_encode_plus(
    batch_text_or_text_pairs = string, 
    add_special_tokens=True, 
    return_attention_mask=True, 
    pad_to_max_length=True, 
    max_length=256, 
    return_tensors='pt'
)



input_ids_string = encoded_string['input_ids']
attention_masks_string = encoded_string['attention_mask']
#labels_string = torch.tensor(string)


#dataset_string = TensorDataset(input_ids_string, attention_masks_string, labels_string)

dataloader_string = DataLoader(dataset_string, 
                                   sampler=SequentialSampler(dataset_string), 
                                   batch_size=batch_size)



input_ids_string

_, predictions, true_test = evaluate(dataloader_test)

accuracy_per_class(predictions, true_test)

print(f'weighted f1 score : {f1_score_func(predictions, true_test)}')

"""---

# Model structure
"""

model

"""---

# End User zone
"""

